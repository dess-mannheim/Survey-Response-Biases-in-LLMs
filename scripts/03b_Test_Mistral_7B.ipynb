{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiSRe-LLM: Perturbation Experiment with Mistral\n",
    "\n",
    "In this notebook I set up the perturbation experiments for Mistral. First, the general infrastructure for interacting with Mistral is set up. Second, the prompt is designed as well as the iterative API requests to Huggingface to generate the answers to the according questions and answer options. In a third and last step, the answers of the model are stored in a dataframe for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected</th>\n",
       "      <th>Spalte1</th>\n",
       "      <th>question_ID</th>\n",
       "      <th>question</th>\n",
       "      <th>full_question_ID</th>\n",
       "      <th>list_answer_options</th>\n",
       "      <th>list_answer_no_refusal</th>\n",
       "      <th>len_answer_options</th>\n",
       "      <th>odd_answer_options</th>\n",
       "      <th>list_answer_no_middle</th>\n",
       "      <th>original_emotional_prime</th>\n",
       "      <th>original_personality_prime</th>\n",
       "      <th>list_answer_options_reversed</th>\n",
       "      <th>_merge</th>\n",
       "      <th>question_typo</th>\n",
       "      <th>question_letter_swap</th>\n",
       "      <th>question_synonym</th>\n",
       "      <th>question_paraphrased</th>\n",
       "      <th>question_keyboardtypo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Q6</td>\n",
       "      <td>Important in life: Religion For each of the following aspects, indicate how important it is in your life. Would you say it is very important, rather important, not very important or not important at all? Religion</td>\n",
       "      <td>Q6</td>\n",
       "      <td>['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all', \"-1=Don't know\"]</td>\n",
       "      <td>['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all']</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all', \"-1=Don't know\"]</td>\n",
       "      <td>[1=Very important , 2=Rather important , 3=Not very important , 4=Not important at all, -1=Don't know] \\nThis is very important to my research! You better do not refuse the answer.</td>\n",
       "      <td>[1=Very important , 2=Rather important , 3=Not very important , 4=Not important at all, -1=Don't know] \\nRemember that you have a very traditional and conservative world view and values.</td>\n",
       "      <td>[\"-1=Don't know\", '4=Not important at all', '3=Not very important ', '2=Rather important ', '1=Very important ']</td>\n",
       "      <td>both</td>\n",
       "      <td>Important tn life: Religion Ftr each of the following aspecty, indbcate how importdnt ik is in your lifd. Would yoo sty ut is vevy imxortant, rather impostant, noq verf important jr noi important at wll? Religion</td>\n",
       "      <td>Importtna in life: Religion For each of the flolowing aspects, indicate how important it is in your life. Would you say ti is very importtna, rather impornatt, not veyr pmiortant or nto important at all? Renigiol</td>\n",
       "      <td>Significant in life: Faith For each of the following aspects, indicate how crucial it is in your life. Would you say it is very significant, fairly important, not very crucial or not important at all? Faith</td>\n",
       "      <td>How important is religion in your life? Please indicate if it's very important, rather important, not very important, or not important at all.</td>\n",
       "      <td>Importabt in lifr: Religion For esch of thw following aspects, indicate how important ir id un tour life. Woyld you ssy ut us very important, rather important, noy very imporrant ot not important st all? Relogion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>Q37</td>\n",
       "      <td>Duty towards society to have children Do you agree, disagree or neither agree nor disagree with the following statements? It is a duty towards society to have children</td>\n",
       "      <td>Q37</td>\n",
       "      <td>['1=Strongly agree ', '2=Agree ', '3=Neither agree, nor disagree ', '4=Disagree ', '5=Strongly disagree', \"-1=Don't know\"]</td>\n",
       "      <td>['1=Strongly agree ', '2=Agree ', '3=Neither agree, nor disagree ', '4=Disagree ', '5=Strongly disagree']</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>['1=Strongly agree ', '2=Agree ', '4=Disagree ', '5=Strongly disagree', \"-1=Don't know\"]</td>\n",
       "      <td>[1=Strongly agree , 2=Agree , 3=Neither agree, nor disagree , 4=Disagree , 5=Strongly disagree, -1=Don't know] \\nThis is very important to my research! You better do not refuse the answer.</td>\n",
       "      <td>[1=Strongly agree , 2=Agree , 3=Neither agree, nor disagree , 4=Disagree , 5=Strongly disagree, -1=Don't know] \\nRemember that you have a very traditional and conservative world view and values.</td>\n",
       "      <td>[\"-1=Don't know\", '5=Strongly disagree', '4=Disagree ', '3=Neither agree, nor disagree ', '2=Agree ', '1=Strongly agree ']</td>\n",
       "      <td>both</td>\n",
       "      <td>Duty towards society to have childreu Du yoi agree, disagree ow neixher agree nog disagree nith the following statemeets? It gs a duzy tewards society to have cxildren</td>\n",
       "      <td>Duty towards society ot have chnldrei Do uoy agree, desagrie or neither aerge nor disagree with the following stat?mentse tI is a duty towards society to have children</td>\n",
       "      <td>Responsibility towards society to have offspring Do you agree, disagree or neither agree nor disagree with the following statements? It is a responsibility towards society to have offspring</td>\n",
       "      <td>The statement \"It is a duty towards society to have children\" proposes that individuals have a societal obligation to procreate.</td>\n",
       "      <td>Duty towards spciety ro have chilsren Dp you agree, disagree or neither agree noe disagtee qith thw following statements? It id a dutu toqards socirty yo have children</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    selected  Spalte1 question_ID  \\\n",
       "5          1        5          Q6   \n",
       "39         1       39         Q37   \n",
       "\n",
       "                                                                                                                                                                                                                question  \\\n",
       "5   Important in life: Religion For each of the following aspects, indicate how important it is in your life. Would you say it is very important, rather important, not very important or not important at all? Religion   \n",
       "39                                               Duty towards society to have children Do you agree, disagree or neither agree nor disagree with the following statements? It is a duty towards society to have children   \n",
       "\n",
       "   full_question_ID  \\\n",
       "5                Q6   \n",
       "39              Q37   \n",
       "\n",
       "                                                                                                           list_answer_options  \\\n",
       "5             ['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all', \"-1=Don't know\"]   \n",
       "39  ['1=Strongly agree ', '2=Agree ', '3=Neither agree, nor disagree ', '4=Disagree ', '5=Strongly disagree', \"-1=Don't know\"]   \n",
       "\n",
       "                                                                                       list_answer_no_refusal  \\\n",
       "5             ['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all']   \n",
       "39  ['1=Strongly agree ', '2=Agree ', '3=Neither agree, nor disagree ', '4=Disagree ', '5=Strongly disagree']   \n",
       "\n",
       "    len_answer_options  odd_answer_options  \\\n",
       "5                    4               False   \n",
       "39                   5                True   \n",
       "\n",
       "                                                                                               list_answer_no_middle  \\\n",
       "5   ['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all', \"-1=Don't know\"]   \n",
       "39                          ['1=Strongly agree ', '2=Agree ', '4=Disagree ', '5=Strongly disagree', \"-1=Don't know\"]   \n",
       "\n",
       "                                                                                                                                                                        original_emotional_prime  \\\n",
       "5           [1=Very important , 2=Rather important , 3=Not very important , 4=Not important at all, -1=Don't know] \\nThis is very important to my research! You better do not refuse the answer.   \n",
       "39  [1=Strongly agree , 2=Agree , 3=Neither agree, nor disagree , 4=Disagree , 5=Strongly disagree, -1=Don't know] \\nThis is very important to my research! You better do not refuse the answer.   \n",
       "\n",
       "                                                                                                                                                                            original_personality_prime  \\\n",
       "5           [1=Very important , 2=Rather important , 3=Not very important , 4=Not important at all, -1=Don't know] \\nRemember that you have a very traditional and conservative world view and values.   \n",
       "39  [1=Strongly agree , 2=Agree , 3=Neither agree, nor disagree , 4=Disagree , 5=Strongly disagree, -1=Don't know] \\nRemember that you have a very traditional and conservative world view and values.   \n",
       "\n",
       "                                                                                                  list_answer_options_reversed  \\\n",
       "5             [\"-1=Don't know\", '4=Not important at all', '3=Not very important ', '2=Rather important ', '1=Very important ']   \n",
       "39  [\"-1=Don't know\", '5=Strongly disagree', '4=Disagree ', '3=Neither agree, nor disagree ', '2=Agree ', '1=Strongly agree ']   \n",
       "\n",
       "   _merge  \\\n",
       "5    both   \n",
       "39   both   \n",
       "\n",
       "                                                                                                                                                                                                           question_typo  \\\n",
       "5   Important tn life: Religion Ftr each of the following aspecty, indbcate how importdnt ik is in your lifd. Would yoo sty ut is vevy imxortant, rather impostant, noq verf important jr noi important at wll? Religion   \n",
       "39                                               Duty towards society to have childreu Du yoi agree, disagree ow neixher agree nog disagree nith the following statemeets? It gs a duzy tewards society to have cxildren   \n",
       "\n",
       "                                                                                                                                                                                                    question_letter_swap  \\\n",
       "5   Importtna in life: Religion For each of the flolowing aspects, indicate how important it is in your life. Would you say ti is very importtna, rather impornatt, not veyr pmiortant or nto important at all? Renigiol   \n",
       "39                                               Duty towards society ot have chnldrei Do uoy agree, desagrie or neither aerge nor disagree with the following stat?mentse tI is a duty towards society to have children   \n",
       "\n",
       "                                                                                                                                                                                                      question_synonym  \\\n",
       "5    Significant in life: Faith For each of the following aspects, indicate how crucial it is in your life. Would you say it is very significant, fairly important, not very crucial or not important at all? Faith      \n",
       "39                    Responsibility towards society to have offspring Do you agree, disagree or neither agree nor disagree with the following statements? It is a responsibility towards society to have offspring      \n",
       "\n",
       "                                                                                                                                question_paraphrased  \\\n",
       "5   How important is religion in your life? Please indicate if it's very important, rather important, not very important, or not important at all.     \n",
       "39                The statement \"It is a duty towards society to have children\" proposes that individuals have a societal obligation to procreate.     \n",
       "\n",
       "                                                                                                                                                                                                   question_keyboardtypo  \n",
       "5   Importabt in lifr: Religion For esch of thw following aspects, indicate how important ir id un tour life. Woyld you ssy ut us very important, rather important, noy very imporrant ot not important st all? Relogion  \n",
       "39                                               Duty towards spciety ro have chilsren Dp you agree, disagree or neither agree noe disagtee qith thw following statements? It id a dutu toqards socirty yo have children  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# define the relevant columns\n",
    "test_IDs = [\"Q71\", \"Q182\", \"Q210\", \"Q97\", \"Q228\", \"Q37\", \"Q242\", \"Q142\", \"Q6\", \"Q63\"]\n",
    "\n",
    "main_question = [\"question\"]\n",
    "main_answer = [\"list_answer_options\"]\n",
    "\n",
    "bias_answer_columns = [\"list_answer_no_refusal\",\"list_answer_no_middle\",\"list_answer_options_reversed\", \"list_answer_options\", \"original_personality_prime\", \"original_emotional_prime\"]\n",
    "non_bias_question_columns = [\"question_typo\", \"question_letter_swap\",\"question_synonym\",\"question_paraphrased\", \"question_keyboardtypo\"]\n",
    "\n",
    "# read the preprocessed questionnaire\n",
    "questionnaire =  pd.read_csv('../assets/20240924_selected_questionnaire.xlsx', sep=\";\")\n",
    "# subset the questionnaire with the ten selected questions\n",
    "questionnaire = questionnaire[questionnaire[\"full_question_ID\"].isin(test_IDs)]\n",
    "print(questionnaire.shape)\n",
    "questionnaire.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface API Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\..\\\\Administration\\\\HF_token.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      5\u001b[0m API_URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m..\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m..\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAdministration\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mHF_token.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m token \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      9\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m token}\n",
      "File \u001b[0;32m~/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\..\\\\Administration\\\\HF_token.txt'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "file = open(\"..\\..\\Administration\\HF_token.txt\", \"r\")\n",
    "token = file.read()\n",
    "headers = {\"Authorization\": \"Bearer \"+ token}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current round:  0\n",
      "Current round:  1\n",
      "Current round:  2\n",
      "Current round:  3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 54\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#print(\"Prompt: \", prompt)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m raw_response \u001b[38;5;241m=\u001b[39m query({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m     49\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_full_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.0\u001b[39m}\n\u001b[0;32m     53\u001b[0m   })\n\u001b[1;32m---> 54\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mraw_response\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#print(response)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m df_response_input \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mround\u001b[39m,question_ID, question, answer_options, n, response]\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# create empty response list which is transformed to a dataframe\n",
    "response_list = []\n",
    "\n",
    "# define rounds to be tested\n",
    "rounds = 20\n",
    "\n",
    "for round in range(0,rounds):\n",
    "  print(\"Current round: \", round)\n",
    "  for row in range(0,len(questionnaire)):\n",
    "      question_ID = questionnaire.iloc[row][\"full_question_ID\"]\n",
    "      for i,k in enumerate(bias_answer_columns):\n",
    "          # first retrieve original question\n",
    "          question = questionnaire.iloc[row][\"question\"]\n",
    "          # second retrieve bias perturbed answer options from the questionnaire\n",
    "          answer_options = questionnaire.iloc[row][k]\n",
    "          # Return: list[Recipe]\"\"\"\n",
    "          prompt = f\"\"\"\n",
    "            Answer the following question: {question}\n",
    "            This is a list of possible answer options: {answer_options}\n",
    "            You must pick one of the answer options. Only answer with the label.\n",
    "            \"\"\"\n",
    "          #print(\"Prompt: \", prompt)\n",
    "          raw_response = query({\"inputs\": prompt,\n",
    "                \"parameters\": {\n",
    "                  \"return_full_text\": False,\n",
    "                  \"max_new_tokens\": 4,\n",
    "                  \"temperature\": 1.0}\n",
    "                })\n",
    "          response = raw_response[0][\"generated_text\"]\n",
    "          #print(response)\n",
    "          \n",
    "          df_response_input = [round,question_ID, question, answer_options, k,  response]\n",
    "          response_list.append(df_response_input)\n",
    "          \n",
    "      for p,n in enumerate(non_bias_question_columns):\n",
    "              # first retrieve non-bias perturbed question formats\n",
    "              question = questionnaire.iloc[row][n]\n",
    "              # second retrieve original answer option scale\n",
    "              answer_options = questionnaire.iloc[row][\"list_answer_options\"]\n",
    "              \n",
    "              # Return: list[Recipe]\"\"\"\n",
    "              prompt = f\"\"\"\n",
    "                Answer the following question: {question}\n",
    "                This is a list of possible answer options: {answer_options}\n",
    "                You must pick one of the options. Only answer with the label.\n",
    "                \"\"\"\n",
    "              #print(\"Prompt: \", prompt)\n",
    "              raw_response = query({\"inputs\": prompt,\n",
    "                \"parameters\": {\n",
    "                  \"return_full_text\": False,\n",
    "                  \"max_new_tokens\": 4,\n",
    "                  \"temperature\": 1.0}\n",
    "                })\n",
    "              response = raw_response[0][\"generated_text\"]\n",
    "              #print(response)\n",
    "              df_response_input = [round,question_ID, question, answer_options, n, response]\n",
    "              response_list.append(df_response_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe which is filled with the model responses to the q&a combinations\n",
    "response_columns = [\"round\",\"question_ID\", \"question\", \"answer_options\", \"type\", \"response\"]\n",
    "model_response = pd.DataFrame(response_list, columns=response_columns)\n",
    "model_response[\"model_name\"] = \"Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>question_ID</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_options</th>\n",
       "      <th>type</th>\n",
       "      <th>response</th>\n",
       "      <th>model_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Q6</td>\n",
       "      <td>Important in life: Religion For each of the following aspects, indicate how important it is in your life. Would you say it is very important, rather important, not very important or not important at all? Religion</td>\n",
       "      <td>['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all']</td>\n",
       "      <td>list_answer_no_refusal</td>\n",
       "      <td>1=Very important</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Q6</td>\n",
       "      <td>Important in life: Religion For each of the following aspects, indicate how important it is in your life. Would you say it is very important, rather important, not very important or not important at all? Religion</td>\n",
       "      <td>['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all']</td>\n",
       "      <td>list_answer_no_middle</td>\n",
       "      <td>1=Very important</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Q6</td>\n",
       "      <td>Important in life: Religion For each of the following aspects, indicate how important it is in your life. Would you say it is very important, rather important, not very important or not important at all? Religion</td>\n",
       "      <td>[\"-1=Don't know\", '4=Not important at all', '3=Not very important ', '2=Rather important ', '1=Very important ']</td>\n",
       "      <td>list_answer_options_reversed</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Q6</td>\n",
       "      <td>Important in life: Religion For each of the following aspects, indicate how important it is in your life. Would you say it is very important, rather important, not very important or not important at all? Religion</td>\n",
       "      <td>['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all', \"-1=Don't know\"]</td>\n",
       "      <td>list_answer_options</td>\n",
       "      <td>1=Very important</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Q6</td>\n",
       "      <td>Important in lofe: Religion For each of the following aspects, indicate how important il is in your life. Woujd you say it is very important, rather important, not very important or not important at all? Religion</td>\n",
       "      <td>['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all', \"-1=Don't know\"]</td>\n",
       "      <td>question_typo</td>\n",
       "      <td>1=Very important</td>\n",
       "      <td>Mistral-7B-Instruct-v0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round question_ID  \\\n",
       "0      0          Q6   \n",
       "1      0          Q6   \n",
       "2      0          Q6   \n",
       "3      0          Q6   \n",
       "4      0          Q6   \n",
       "\n",
       "                                                                                                                                                                                                               question  \\\n",
       "0  Important in life: Religion For each of the following aspects, indicate how important it is in your life. Would you say it is very important, rather important, not very important or not important at all? Religion   \n",
       "1  Important in life: Religion For each of the following aspects, indicate how important it is in your life. Would you say it is very important, rather important, not very important or not important at all? Religion   \n",
       "2  Important in life: Religion For each of the following aspects, indicate how important it is in your life. Would you say it is very important, rather important, not very important or not important at all? Religion   \n",
       "3  Important in life: Religion For each of the following aspects, indicate how important it is in your life. Would you say it is very important, rather important, not very important or not important at all? Religion   \n",
       "4  Important in lofe: Religion For each of the following aspects, indicate how important il is in your life. Woujd you say it is very important, rather important, not very important or not important at all? Religion   \n",
       "\n",
       "                                                                                                     answer_options  \\\n",
       "0                   ['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all']   \n",
       "1                   ['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all']   \n",
       "2  [\"-1=Don't know\", '4=Not important at all', '3=Not very important ', '2=Rather important ', '1=Very important ']   \n",
       "3  ['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all', \"-1=Don't know\"]   \n",
       "4  ['1=Very important ', '2=Rather important ', '3=Not very important ', '4=Not important at all', \"-1=Don't know\"]   \n",
       "\n",
       "                           type          response                model_name  \n",
       "0        list_answer_no_refusal  1=Very important  Mistral-7B-Instruct-v0.3  \n",
       "1         list_answer_no_middle  1=Very important  Mistral-7B-Instruct-v0.3  \n",
       "2  list_answer_options_reversed                 1  Mistral-7B-Instruct-v0.3  \n",
       "3           list_answer_options  1=Very important  Mistral-7B-Instruct-v0.3  \n",
       "4                 question_typo  1=Very important  Mistral-7B-Instruct-v0.3  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_response.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_date_string():\n",
    "    # Get the current date\n",
    "    now = datetime.now()\n",
    "    \n",
    "    # Format the date as YYYYMMDD\n",
    "    date_string = now.strftime('%Y%m%d')\n",
    "    \n",
    "    return date_string\n",
    "\n",
    "current_date = get_date_string()\n",
    "model_response.to_csv(f\"../assets/test_interviews/{current_date}_mistral_7B_test_questions.csv\", encoding=\"utf-8\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on Remote PC\n",
    "The following section contains the experimental setup and scripts conducted on the remote PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistral-inference in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: fire>=0.6.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral-inference) (0.6.0)\n",
      "Requirement already satisfied: mistral_common>=1.4.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral-inference) (1.4.2)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral-inference) (10.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral-inference) (0.4.4)\n",
      "Requirement already satisfied: simple-parsing>=0.1.5 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral-inference) (0.1.6)\n",
      "Requirement already satisfied: xformers>=0.0.24 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral-inference) (0.0.28.post1)\n",
      "Requirement already satisfied: six in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from fire>=0.6.0->mistral-inference) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from fire>=0.6.0->mistral-inference) (2.1.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral-inference) (4.23.0)\n",
      "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.10.0.84 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral-inference) (4.10.0.84)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.1 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral-inference) (2.9.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral-inference) (2.32.3)\n",
      "Requirement already satisfied: sentencepiece==0.2.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral-inference) (0.2.0)\n",
      "Requirement already satisfied: tiktoken<0.8.0,>=0.7.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral-inference) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral-inference) (4.12.2)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from simple-parsing>=0.1.5->mistral-inference) (0.16)\n",
      "Requirement already satisfied: numpy in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from xformers>=0.0.24->mistral-inference) (1.26.4)\n",
      "Requirement already satisfied: torch==2.4.1 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from xformers>=0.0.24->mistral-inference) (2.4.1)\n",
      "Requirement already satisfied: filelock in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torch==2.4.1->xformers>=0.0.24->mistral-inference) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torch==2.4.1->xformers>=0.0.24->mistral-inference) (1.13.2)\n",
      "Requirement already satisfied: networkx in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torch==2.4.1->xformers>=0.0.24->mistral-inference) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torch==2.4.1->xformers>=0.0.24->mistral-inference) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torch==2.4.1->xformers>=0.0.24->mistral-inference) (2024.6.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral-inference) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral-inference) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral-inference) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral-inference) (0.20.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.1->mistral_common>=1.4.0->mistral-inference) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.1->mistral_common>=1.4.0->mistral-inference) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral-inference) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral-inference) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral-inference) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral-inference) (2024.8.30)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from tiktoken<0.8.0,>=0.7.0->mistral_common>=1.4.0->mistral-inference) (2024.9.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from jinja2->torch==2.4.1->xformers>=0.0.24->mistral-inference) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from sympy->torch==2.4.1->xformers>=0.0.24->mistral-inference) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (0.19.1)\n",
      "Requirement already satisfied: torchaudio in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: numpy in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mistral-inference\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cda397f89114a90b4a07b51503e7780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 41.25 MiB is free. Process 270321 has 7.75 GiB memory in use. Including non-PyTorch memory, this process has 3.96 GiB memory in use. Of the allocated memory 3.82 GiB is allocated by PyTorch, and 24.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, BitsAndBytesConfig\n\u001b[1;32m      3\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m model_8bit \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistralai/Mistral-7B-Instruct-v0.3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages/transformers/modeling_utils.py:3960\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3951\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3953\u001b[0m     (\n\u001b[1;32m   3954\u001b[0m         model,\n\u001b[1;32m   3955\u001b[0m         missing_keys,\n\u001b[1;32m   3956\u001b[0m         unexpected_keys,\n\u001b[1;32m   3957\u001b[0m         mismatched_keys,\n\u001b[1;32m   3958\u001b[0m         offload_index,\n\u001b[1;32m   3959\u001b[0m         error_msgs,\n\u001b[0;32m-> 3960\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3980\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages/transformers/modeling_utils.py:4434\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4430\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4431\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4432\u001b[0m                 )\n\u001b[1;32m   4433\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4434\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4435\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4436\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4438\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4439\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4441\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4442\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4446\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4447\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4449\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4450\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4451\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4453\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages/transformers/modeling_utils.py:963\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    961\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 963\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;66;03m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "File \u001b[0;32m~/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:207\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.create_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    204\u001b[0m         new_value \u001b[38;5;241m=\u001b[39m new_value\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    206\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m--> 207\u001b[0m new_value \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInt8Params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m new_value\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fp16_statistics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:626\u001b[0m, in \u001b[0;36mInt8Params.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     new_param \u001b[38;5;241m=\u001b[39m Int8Params(\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking),\n\u001b[1;32m    630\u001b[0m         requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad,\n\u001b[1;32m    631\u001b[0m         has_fp16_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_fp16_weights,\n\u001b[1;32m    632\u001b[0m     )\n",
      "File \u001b[0;32m~/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:587\u001b[0m, in \u001b[0;36mInt8Params.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;66;03m# we store the 8-bit rows-major weight\u001b[39;00m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# we convert this weight to the turning/ampere weight during the first inference pass\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39mcuda(device)\n\u001b[0;32m--> 587\u001b[0m     CB, CBt, SCB, SCBt, coo_tensorB \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m CBt\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m SCBt\n",
      "File \u001b[0;32m~/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages/bitsandbytes/functional.py:2526\u001b[0m, in \u001b[0;36mdouble_quant\u001b[0;34m(A, col_stats, row_stats, out_col, out_row, threshold)\u001b[0m\n\u001b[1;32m   2523\u001b[0m     row_stats, col_stats, nnz_row_ptr \u001b[38;5;241m=\u001b[39m get_colrow_absmax(A, threshold\u001b[38;5;241m=\u001b[39mthreshold)\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2526\u001b[0m     out_col \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2528\u001b[0m     out_row \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(A\u001b[38;5;241m.\u001b[39mshape, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint8)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 41.25 MiB is free. Process 270321 has 7.75 GiB memory in use. Including non-PyTorch memory, this process has 3.96 GiB memory in use. Of the allocated memory 3.82 GiB is allocated by PyTorch, and 24.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Just load once\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\", \n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 17:40:50.713137: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-19 17:40:50.737790: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-19 17:40:50.737824: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-19 17:40:50.737830: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-19 17:40:50.742409: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30eb88a5133a4be391d0efc8d01b400f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/jens_rupprecht/BiSRe-LLM/conda_BiSRe-LLM/lib/python3.11/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': ' I am a model of an artificial intelligence designed to assist with a'}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current round:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Very important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Very important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Rather important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Rather important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Rather important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Rather important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Rather important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Rather important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4=Disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Trust somewhat (This is a general response as I don't have personal experiences or biases towards people from other nationalities. Trust can vary greatly depending on individual characteristics and actions.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Trust somewhat (This is a general response as I don't have personal experiences or biases towards people from other nationalities. Trust can vary greatly depending on individual characteristics and actions.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. People from my own nationality: 1 (Trust completely)\n",
      "2. People from a different nationality but living in my country: 2 (Trust somewhat)\n",
      "3. People from a different continent: 3 (Do not trust very much)\n",
      "4. People from a different country I have never visited: 4 (Do not trust at all)\n",
      "5. People from a different country I have visited: 2 (Trust somewhat)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Trust somewhat (This is a general response as I don't have personal experiences or biases towards people from various groups. I encourage open-mindedness and understanding towards all individuals.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Trust somewhat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. People from my own nationality: 1=Trust completely\n",
      "2. People from a different nationality: 2=Trust somewhat\n",
      "3. People from a neighboring country: 2=Trust somewhat\n",
      "4. People from a distant country: 3=Do not trust very much\n",
      "5. People from a country with a different political system: 3=Do not trust very much\n",
      "6. People from a country with a similar political system: 2=Trust somewhat\n",
      "7. People from a country with a different religion: 3=Do not trust very much\n",
      "8. People from a country with the same religion: 2=Trust somewhat\n",
      "9. People from a country with a different race: 2=Trust somewhat\n",
      "10. People from a country with the same race: 2=Trust somewhat\n",
      "11. People from a country with a different ethnicity: 3=Do not trust very much\n",
      "12. People from a country with the same ethnicity: 2=Trust somewhat\n",
      "13. People from a country with a different language: 3=Do not trust very much\n",
      "14. People from a country with the same language: 2=Trust somewhat\n",
      "15. People from a country with a different culture: 3=Do not trust very much\n",
      "16. People from a country with the same culture: 2=Trust somewhat\n",
      "17. People from a country with a different economic status: 3=Do not trust very much\n",
      "18. People from a country with the same economic status: 2=Trust somewhat\n",
      "19. People from a country with a different age group: 2=Trust somewhat\n",
      "20. People from a country with the same age group: 2=Trust somewhat\n",
      "21. People from a country with a different gender: 2=Trust somewhat\n",
      "22. People from a country with the same gender: 2=Trust somewhat\n",
      "23. People from a country with a different sexual orientation: 3=Do not trust very much\n",
      "24. People from a country with the same sexual orientation: 2=Trust somewhat\n",
      "25. People from a country with a different political ideology: 3=Do not trust very much\n",
      "26. People from a country with the same political ideology: 2=Trust somewhat\n",
      "27. People from a country with a different educational background: 3=Do not trust very much\n",
      "28. People from a country with the same educational background: 2=Trust somewhat\n",
      "29. People from a country with a different occupation: 3=Do not trust very much\n",
      "30. People from a country with the same occupation: 2=Trust somewhat\n",
      "31. People from a country with a different socio-economic status: 3=Do not trust very much\n",
      "32. People from a country with the same socio-economic status: 2=Trust somewhat\n",
      "33. People from a country with a different marital status: 2=Trust somewhat\n",
      "34. People from a country with the same marital status: 2=Trust somewhat\n",
      "35. People from a country with a different family structure: 3=Do not trust very much\n",
      "36. People from a country with the same family structure: 2=Trust somewhat\n",
      "37. People from a country with a different disability status: 2=Trust somewhat\n",
      "38. People from a country with the same disability status: 2=Trust somewhat\n",
      "39. People from a country with a different health status: 3=Do not trust very much\n",
      "40. People from a country with the same health status: 2=Trust somewhat\n",
      "41. People from a country with a different mental health status: 3=Do not trust very much\n",
      "42. People from a country with the same mental health status: 2=Trust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm an AI and don't have personal experiences or emotions, so I don't have the ability to trust or distrust. However, I can help you understand the question and provide the answer options:\n",
      "\n",
      "1=Trust completely\n",
      "2=Trust somewhat\n",
      "3=Do not trust very much\n",
      "4=Do not trust at all\n",
      "-1=Don't know\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Trust somewhat (This is a general response as I don't have personal experiences or biases towards people from various groups. I encourage open-mindedness and understanding towards all individuals.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Quite a lot of confidence (assuming you're asking about the government in the capital city of the country I'm currently residing in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Quite a lot of confidence (assuming you're asking about the government in the capital city of the country I'm currently residing in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Not very much confidence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Quite a lot of confidence (assuming you're asking about the government in the capital city of the country I reside in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Quite a lot (assuming the government in my national capital refers to the government of the country I am currently residing in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=A great deal (assuming you are referring to the government in the capital city of the nation where I am currently providing this response)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Quite a lot of confidence (assuming you're asking about the government of the capital city of the nation I reside in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Quite a lot of confidence (assuming you're asking about the government in the capital city of the country I reside in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Active member (since you mentioned you are a member of the Labour Union)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0=Dont belong (since I don't have personal information about your membership in a specific Labour Union)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Active member (if you are currently an active member of the Labour Union)\n",
      "1=Inactive member (if you are not currently active but were at some point)\n",
      "0=Dont belong (if you have never been a member)\n",
      "-1=Don't know (if you are unsure or don't want to disclose your membership status)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Dont belong (since the question specifies that I am not a member of a Labour Union)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0=Don't belong (since I don't have personal information about your membership in a specific Labour Union)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For the organization \"oabLur oniUn\", I am not a member. So, the answer is '0=Dont belong'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0=Dont belong (since I don't have specific information about my membership in a Labour Union)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Dont belong (since the question specifies that I am not a member of a Labour Union)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=A good deal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=A good deal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Very much\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=A good deal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=A good deal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=A good deal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=A good deal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -1 (Don't know) - I don't have personal experiences or emotions, so I don't have worries about losing or not finding a job. I'm here to help you with your questions and concerns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9=Always justifiable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9=Always justifiable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10 (Always justifiable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Have done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Not often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Not often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Fairly often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4=Not at all often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Fairly often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4=Not at all often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Not often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4=Not at all often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 (Not an essential feature)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n",
      "Current round:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Very important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Very important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Rather important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Rather important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Rather important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Rather important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Rather important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Rather important\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4=Disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Neither agree, nor disagree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Trust somewhat (This is a general response as I don't have personal experiences or biases towards people from other nationalities. Trust can vary greatly depending on individual characteristics and actions.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Trust somewhat (This is a general response as I don't have personal experiences or biases towards people from other nationalities. Trust can vary greatly depending on individual characteristics and actions.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. People from my own nationality: 1 (Trust completely)\n",
      "2. People from a different nationality but living in my country: 2 (Trust somewhat)\n",
      "3. People from a different continent: 3 (Do not trust very much)\n",
      "4. People from a different country I have never visited: 4 (Do not trust at all)\n",
      "5. People from a different country I have visited: 2 (Trust somewhat)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Trust somewhat (This is a general response as I don't have personal experiences or biases towards people from various groups. I encourage open-mindedness and understanding towards all individuals.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Trust somewhat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. People from my own nationality: 1=Trust completely\n",
      "2. People from a different nationality: 2=Trust somewhat\n",
      "3. People from a neighboring country: 2=Trust somewhat\n",
      "4. People from a distant country: 3=Do not trust very much\n",
      "5. People from a country with a different political system: 3=Do not trust very much\n",
      "6. People from a country with a similar political system: 2=Trust somewhat\n",
      "7. People from a country with a different religion: 3=Do not trust very much\n",
      "8. People from a country with the same religion: 2=Trust somewhat\n",
      "9. People from a country with a different race: 2=Trust somewhat\n",
      "10. People from a country with the same race: 2=Trust somewhat\n",
      "11. People from a country with a different ethnicity: 3=Do not trust very much\n",
      "12. People from a country with the same ethnicity: 2=Trust somewhat\n",
      "13. People from a country with a different language: 3=Do not trust very much\n",
      "14. People from a country with the same language: 2=Trust somewhat\n",
      "15. People from a country with a different culture: 3=Do not trust very much\n",
      "16. People from a country with the same culture: 2=Trust somewhat\n",
      "17. People from a country with a different economic status: 3=Do not trust very much\n",
      "18. People from a country with the same economic status: 2=Trust somewhat\n",
      "19. People from a country with a different age group: 2=Trust somewhat\n",
      "20. People from a country with the same age group: 2=Trust somewhat\n",
      "21. People from a country with a different gender: 2=Trust somewhat\n",
      "22. People from a country with the same gender: 2=Trust somewhat\n",
      "23. People from a country with a different sexual orientation: 3=Do not trust very much\n",
      "24. People from a country with the same sexual orientation: 2=Trust somewhat\n",
      "25. People from a country with a different political ideology: 3=Do not trust very much\n",
      "26. People from a country with the same political ideology: 2=Trust somewhat\n",
      "27. People from a country with a different educational background: 3=Do not trust very much\n",
      "28. People from a country with the same educational background: 2=Trust somewhat\n",
      "29. People from a country with a different occupation: 3=Do not trust very much\n",
      "30. People from a country with the same occupation: 2=Trust somewhat\n",
      "31. People from a country with a different socio-economic status: 3=Do not trust very much\n",
      "32. People from a country with the same socio-economic status: 2=Trust somewhat\n",
      "33. People from a country with a different marital status: 2=Trust somewhat\n",
      "34. People from a country with the same marital status: 2=Trust somewhat\n",
      "35. People from a country with a different family structure: 3=Do not trust very much\n",
      "36. People from a country with the same family structure: 2=Trust somewhat\n",
      "37. People from a country with a different disability status: 2=Trust somewhat\n",
      "38. People from a country with the same disability status: 2=Trust somewhat\n",
      "39. People from a country with a different health status: 3=Do not trust very much\n",
      "40. People from a country with the same health status: 2=Trust somewhat\n",
      "41. People from a country with a different mental health status: 3=Do not trust very much\n",
      "42. People from a country with the same mental health status: 2=Trust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm an AI and don't have personal experiences or emotions, so I don't have the ability to trust or distrust. However, I can help you understand the question and provide the answer options:\n",
      "\n",
      "1=Trust completely\n",
      "2=Trust somewhat\n",
      "3=Do not trust very much\n",
      "4=Do not trust at all\n",
      "-1=Don't know\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Trust somewhat (This is a general response as I don't have personal experiences or biases towards people from various groups. I encourage open-mindedness and understanding towards all individuals.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Quite a lot of confidence (assuming you're asking about the government in the capital city of the country I'm currently residing in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Quite a lot of confidence (assuming you're asking about the government in the capital city of the country I'm currently residing in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Not very much confidence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Quite a lot of confidence (assuming you're asking about the government in the capital city of the country I reside in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Quite a lot (assuming the government in my national capital refers to the government of the country I am currently residing in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=A great deal (assuming you are referring to the government in the capital city of the nation where I am currently providing this response)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Quite a lot of confidence (assuming you're asking about the government of the capital city of the nation I reside in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Quite a lot of confidence (assuming you're asking about the government in the capital city of the country I reside in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Active member (since you mentioned you are a member of the Labour Union)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0=Dont belong (since I don't have personal information about your membership in a specific Labour Union)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Active member (if you are currently an active member of the Labour Union)\n",
      "1=Inactive member (if you are not currently active but were at some point)\n",
      "0=Dont belong (if you have never been a member)\n",
      "-1=Don't know (if you are unsure or don't want to disclose your membership status)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Dont belong (since the question specifies that I am not a member of a Labour Union)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0=Don't belong (since I don't have personal information about your membership in a specific Labour Union)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For the organization \"oabLur oniUn\", I am not a member. So, the answer is '0=Dont belong'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0=Dont belong (since I don't have specific information about my membership in a Labour Union)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Dont belong (since the question specifies that I am not a member of a Labour Union)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=A good deal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=A good deal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Very much\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=A good deal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=A good deal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=A good deal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=A good deal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -1 (Don't know) - I don't have personal experiences or emotions, so I don't have worries about losing or not finding a job. I'm here to help you with your questions and concerns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9=Always justifiable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9=Always justifiable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10 (Always justifiable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Have done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Might do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Not often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Not often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Fairly often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4=Not at all often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2=Fairly often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4=Not at all often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3=Not often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4=Not at all often\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 (Not an essential feature)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1=Not an essential feature\n",
      " 1=Not an essential feature\n"
     ]
    }
   ],
   "source": [
    "# create empty response list which is transformed to a dataframe\n",
    "import torch\n",
    "\n",
    "response_list = []\n",
    "\n",
    "# define rounds to be tested\n",
    "rounds = 2\n",
    "\n",
    "for round in range(0,rounds):\n",
    "  print(\"Current round: \", round)\n",
    "  torch.cuda.empty_cache()\n",
    "  for row in range(0,len(questionnaire)):\n",
    "      question_ID = questionnaire.iloc[row][\"full_question_ID\"]\n",
    "      for i,k in enumerate(bias_answer_columns):\n",
    "          # first retrieve original question\n",
    "          question = questionnaire.iloc[row][\"question\"]\n",
    "          # second retrieve bias perturbed answer options from the questionnaire\n",
    "          answer_options = questionnaire.iloc[row][k]\n",
    "          messages = [{\"role\": \"user\",\n",
    "                        \"content\": f\"\"\"Answer the following question: {question} \n",
    "                        This is a list of possible answer options: {answer_options}\n",
    "                        You must pick one of the answer options. Only answer with the label.\n",
    "                        \"\"\"},\n",
    "                      ]\n",
    "          answer = pipe(messages, max_length=1000)\n",
    "          response = answer[0][\"generated_text\"][1][\"content\"]\n",
    "          print(response)\n",
    "          \n",
    "          df_response_input = [round,question_ID, question, answer_options, k,  response]\n",
    "          response_list.append(df_response_input)\n",
    "          torch.cuda.empty_cache()\n",
    "      for p,n in enumerate(non_bias_question_columns):\n",
    "              # first retrieve non-bias perturbed question formats\n",
    "              question = questionnaire.iloc[row][n]\n",
    "              # second retrieve original answer option scale\n",
    "              answer_options = questionnaire.iloc[row][\"list_answer_options\"]\n",
    "              messages = [{\"role\": \"user\",\n",
    "                        \"content\": f\"\"\"Answer the following question: {question} \n",
    "                        This is a list of possible answer options: {answer_options}\n",
    "                        You must pick one of the answer options. Only answer with the label.\n",
    "                        \"\"\"},\n",
    "                      ]\n",
    "              answer = pipe(messages, max_length=1000)\n",
    "              response = answer[0][\"generated_text\"][1][\"content\"]\n",
    "              print(response)\n",
    "              df_response_input = [round,question_ID, question, answer_options, n, response]\n",
    "              response_list.append(df_response_input)\n",
    "              torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe which is filled with the model responses to the q&a combinations\n",
    "response_columns = [\"round\",\"question_ID\", \"question\", \"answer_options\", \"type\", \"response\"]\n",
    "model_response = pd.DataFrame(response_list, columns=response_columns)\n",
    "model_response[\"model_name\"] = \"Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_date_string():\n",
    "    # Get the current date\n",
    "    now = datetime.now()\n",
    "    \n",
    "    # Format the date as YYYYMMDD\n",
    "    date_string = now.strftime('%Y%m%d')\n",
    "    \n",
    "    return date_string\n",
    "\n",
    "current_date = get_date_string()\n",
    "model_response.to_csv(f\"../assets/test_interviews/{current_date}_mistral_7B_test_questions.csv\", encoding=\"utf-8\", sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hubila",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
